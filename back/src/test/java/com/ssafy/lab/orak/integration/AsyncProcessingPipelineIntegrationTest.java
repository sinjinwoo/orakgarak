package com.ssafy.lab.orak.integration;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.ssafy.lab.orak.event.dto.UploadEvent;
import com.ssafy.lab.orak.event.service.EventBridgeService;
import com.ssafy.lab.orak.event.service.KafkaEventConsumer;
import com.ssafy.lab.orak.processing.service.BatchProcessingService;
import com.ssafy.lab.orak.upload.entity.Upload;
import com.ssafy.lab.orak.upload.enums.ProcessingStatus;
import com.ssafy.lab.orak.upload.repository.UploadRepository;
import com.ssafy.lab.orak.upload.service.FileUploadService;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.junit.jupiter.api.DisplayName;
import org.junit.jupiter.api.Test;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.kafka.test.context.EmbeddedKafka;
import org.springframework.mock.web.MockMultipartFile;
import org.springframework.test.annotation.DirtiesContext;
import org.springframework.test.context.ActiveProfiles;
import org.springframework.transaction.annotation.Transactional;

import java.time.LocalDateTime;
import java.util.concurrent.TimeUnit;

import static org.assertj.core.api.Assertions.assertThat;
import static org.awaitility.Awaitility.await;

@SpringBootTest
@ActiveProfiles("test")
@EmbeddedKafka(
        partitions = 1,
        topics = {"test-upload-events", "test-processing-status", "test-processing-results"},
        brokerProperties = {"listeners=PLAINTEXT://localhost:9092", "port=9092"}
)
@DirtiesContext
class AsyncProcessingPipelineIntegrationTest {

    @Autowired
    private FileUploadService fileUploadService;

    @Autowired
    private BatchProcessingService batchProcessingService;

    @Autowired
    private EventBridgeService eventBridgeService;

    @Autowired
    private KafkaEventConsumer kafkaEventConsumer;

    @Autowired
    private UploadRepository uploadRepository;

    @Autowired
    private ObjectMapper objectMapper;

    @Test
    @DisplayName("ì „ì²´ ë¹„ë™ê¸° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸")
    @Transactional
    void testCompleteAsyncProcessingPipeline() throws Exception {
        // ========================================
        // 1ë‹¨ê³„: S3 Presigned URL ì—…ë¡œë“œ ì‹œë®¬ë ˆì´ì…˜
        // ========================================
        System.out.println("ğŸš€ 1ë‹¨ê³„: S3 ì—…ë¡œë“œ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘");

        MockMultipartFile audioFile = new MockMultipartFile(
                "audio",
                "integration-test-audio.mp3",
                "audio/mpeg",
                "test audio content for integration".getBytes()
        );

        Upload uploadedFile = fileUploadService.uploadSingleFile(audioFile, "integration-test", 1L);

        assertThat(uploadedFile.getProcessingStatus()).isEqualTo(ProcessingStatus.UPLOADED);
        System.out.println("âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ: " + uploadedFile.getOriginalFilename());

        // ========================================
        // 2ë‹¨ê³„: EventBridge ì´ë²¤íŠ¸ ë°œì†¡
        // ========================================
        System.out.println("ğŸš€ 2ë‹¨ê³„: EventBridge ì´ë²¤íŠ¸ ë°œì†¡");

        UploadEvent uploadEvent = UploadEvent.builder()
                .eventId("integration-test-event")
                .eventType("UPLOAD_COMPLETED")
                .uploadId(uploadedFile.getId())
                .uploaderId(1L)
                .originalFilename(uploadedFile.getOriginalFilename())
                .fileSize(uploadedFile.getFileSize())
                .contentType(uploadedFile.getContentType())
                .s3Key(uploadedFile.getFullPath())
                .currentStatus(ProcessingStatus.UPLOADED)
                .requiresAudioProcessing(true)
                .requiresImageProcessing(false)
                .eventTime(LocalDateTime.now())
                .build();

        // EventBridge ì´ë²¤íŠ¸ ë°œì†¡ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” S3 ì´ë²¤íŠ¸ê°€ ìë™ íŠ¸ë¦¬ê±°)
        boolean eventSent = eventBridgeService.publishUploadEvent(uploadEvent);
        assertThat(eventSent).isTrue();
        System.out.println("âœ… EventBridge ì´ë²¤íŠ¸ ë°œì†¡ ì™„ë£Œ");

        // ========================================
        // 3ë‹¨ê³„: Kafkaë¥¼ í†µí•œ ì´ë²¤íŠ¸ ì²˜ë¦¬
        // ========================================
        System.out.println("ğŸš€ 3ë‹¨ê³„: Kafka ì´ë²¤íŠ¸ ì²˜ë¦¬");

        // Kafkaë¡œ ì´ë²¤íŠ¸ ë°œì†¡ (EventBridge â†’ Kafka ì—°ë™ ì‹œë®¬ë ˆì´ì…˜)
        String eventJson = objectMapper.writeValueAsString(uploadEvent);

        // ì‹¤ì œë¡œëŠ” EventBridgeê°€ Kafkaë¡œ ì „ë‹¬í•˜ì§€ë§Œ, í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ì§ì ‘ ì „ì†¡
        try (KafkaProducer<String, String> producer = createTestKafkaProducer()) {
            ProducerRecord<String, String> record = new ProducerRecord<>(
                    "test-upload-events",
                    uploadEvent.getUploadId().toString(),
                    eventJson
            );
            producer.send(record).get(); // ë™ê¸°ì  ì „ì†¡
        }

        System.out.println("âœ… Kafka ì´ë²¤íŠ¸ ë°œì†¡ ì™„ë£Œ");

        // Kafka ì´ë²¤íŠ¸ ì²˜ë¦¬ ëŒ€ê¸° (Consumerê°€ ì²˜ë¦¬í•  ì‹œê°„ í™•ë³´)
        await().atMost(10, TimeUnit.SECONDS).untilAsserted(() -> {
            Upload updatedUpload = uploadRepository.findById(uploadedFile.getId()).orElse(null);
            assertThat(updatedUpload).isNotNull();
        });

        // ========================================
        // 4ë‹¨ê³„: ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ ë™ì‘
        // ========================================
        System.out.println("ğŸš€ 4ë‹¨ê³„: ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘");

        // ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
        batchProcessingService.processPendingFiles();

        // ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ëŒ€ê¸°
        await().atMost(15, TimeUnit.SECONDS).untilAsserted(() -> {
            Upload processedUpload = uploadRepository.findById(uploadedFile.getId()).orElse(null);
            assertThat(processedUpload).isNotNull();
            assertThat(processedUpload.getProcessingStatus())
                    .isIn(ProcessingStatus.PROCESSING, ProcessingStatus.COMPLETED);
        });

        System.out.println("âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ");

        // ========================================
        // 5ë‹¨ê³„: ì²˜ë¦¬ ê²°ê³¼ ê²€ì¦
        // ========================================
        System.out.println("ğŸš€ 5ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ê²€ì¦");

        Upload finalUpload = uploadRepository.findById(uploadedFile.getId()).orElse(null);
        assertThat(finalUpload).isNotNull();

        // ì²˜ë¦¬ ìƒíƒœê°€ ì™„ë£Œë˜ê±°ë‚˜ ì§„í–‰ ì¤‘ì¸ì§€ í™•ì¸
        assertThat(finalUpload.getProcessingStatus())
                .isIn(ProcessingStatus.PROCESSING, ProcessingStatus.COMPLETED);

        // í†µê³„ í™•ì¸
        BatchProcessingService.ProcessingStatistics stats =
                batchProcessingService.getStatistics();

        assertThat(stats.getMaxConcurrentJobs()).isGreaterThan(0);

        System.out.println("âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!");
        System.out.println("ğŸ“Š ìµœì¢… í†µê³„:");
        System.out.println("  - íŒŒì¼ ID: " + finalUpload.getId());
        System.out.println("  - ìµœì¢… ìƒíƒœ: " + finalUpload.getProcessingStatus());
        System.out.println("  - í™œì„± ì‘ì—…: " + stats.getActiveJobs());
        System.out.println("  - ìµœëŒ€ ë™ì‹œ ì‘ì—…: " + stats.getMaxConcurrentJobs());
    }

    @Test
    @DisplayName("ëŒ€ìš©ëŸ‰ íŒŒì¼ ë™ì‹œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸")
    @Transactional
    void testHighVolumeAsyncProcessing() throws Exception {
        System.out.println("ğŸš€ ëŒ€ìš©ëŸ‰ ë™ì‹œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘");

        // ì—¬ëŸ¬ íŒŒì¼ ë™ì‹œ ì—…ë¡œë“œ
        for (int i = 1; i <= 10; i++) {
            MockMultipartFile audioFile = new MockMultipartFile(
                    "audio" + i,
                    "volume-test-" + i + ".mp3",
                    "audio/mpeg",
                    ("audio content " + i).getBytes()
            );

            Upload upload = fileUploadService.uploadSingleFile(audioFile, "volume-test", 1L);

            // ê° íŒŒì¼ì— ëŒ€í•œ ì´ë²¤íŠ¸ ìƒì„± ë° ë°œì†¡
            UploadEvent event = UploadEvent.builder()
                    .eventId("volume-event-" + i)
                    .eventType("UPLOAD_COMPLETED")
                    .uploadId(upload.getId())
                    .uploaderId(1L)
                    .originalFilename(upload.getOriginalFilename())
                    .currentStatus(ProcessingStatus.UPLOADED)
                    .requiresAudioProcessing(true)
                    .eventTime(LocalDateTime.now())
                    .build();

            // Kafkaë¡œ ì´ë²¤íŠ¸ ë°œì†¡
            String eventJson = objectMapper.writeValueAsString(event);
            try (KafkaProducer<String, String> producer = createTestKafkaProducer()) {
                ProducerRecord<String, String> record = new ProducerRecord<>(
                        "test-upload-events",
                        "volume-key-" + i,
                        eventJson
                );
                producer.send(record);
            }
        }

        System.out.println("âœ… 10ê°œ íŒŒì¼ ì—…ë¡œë“œ ë° ì´ë²¤íŠ¸ ë°œì†¡ ì™„ë£Œ");

        // ë°°ì¹˜ ì²˜ë¦¬ ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰ (ë™ì‹œ ì²˜ë¦¬ ì œí•œ í…ŒìŠ¤íŠ¸)
        for (int round = 1; round <= 3; round++) {
            System.out.println("ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ ë¼ìš´ë“œ " + round);
            batchProcessingService.processPendingFiles();
            Thread.sleep(2000); // ê° ë¼ìš´ë“œ ê°„ ê°„ê²©
        }

        // ìµœì¢… ê²°ê³¼ í™•ì¸
        await().atMost(30, TimeUnit.SECONDS).untilAsserted(() -> {
            long processingOrCompletedCount = uploadRepository.findAll().stream()
                    .mapToLong(upload -> {
                        ProcessingStatus status = upload.getProcessingStatus();
                        return (status == ProcessingStatus.PROCESSING ||
                                status == ProcessingStatus.COMPLETED) ? 1 : 0;
                    })
                    .sum();

            assertThat(processingOrCompletedCount).isGreaterThanOrEqualTo(3); // ìµœì†Œ 3ê°œëŠ” ì²˜ë¦¬ë˜ì–´ì•¼ í•¨
        });

        // í†µê³„ í™•ì¸
        BatchProcessingService.ProcessingStatistics stats =
                batchProcessingService.getStatistics();

        System.out.println("âœ… ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!");
        System.out.println("ğŸ“Š ìµœì¢… í†µê³„:");
        System.out.println("  - í™œì„± ì‘ì—…: " + stats.getActiveJobs());
        System.out.println("  - ì²˜ë¦¬ ì¤‘ì¸ íŒŒì¼: " + stats.getProcessingCount());
        System.out.println("  - ì™„ë£Œëœ íŒŒì¼: " + stats.getCompletedCount());
        System.out.println("  - ì‹¤íŒ¨í•œ íŒŒì¼: " + stats.getFailedCount());
    }

    @Test
    @DisplayName("ì²˜ë¦¬ ì‹¤íŒ¨ ë° ì¬ì‹œë„ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸")
    @Transactional
    void testFailureAndRetryScenario() throws Exception {
        System.out.println("ğŸš€ ì²˜ë¦¬ ì‹¤íŒ¨ ë° ì¬ì‹œë„ í…ŒìŠ¤íŠ¸ ì‹œì‘");

        // ì‹¤íŒ¨í•  ê°€ëŠ¥ì„±ì´ ìˆëŠ” íŒŒì¼ ì—…ë¡œë“œ
        MockMultipartFile corruptedFile = new MockMultipartFile(
                "corrupted",
                "corrupted-audio.mp3",
                "audio/mpeg",
                "corrupted audio data".getBytes()
        );

        Upload upload = fileUploadService.uploadSingleFile(corruptedFile, "failure-test", 1L);

        // ì²˜ë¦¬ ì‹¤íŒ¨ ì´ë²¤íŠ¸ ìƒì„±
        UploadEvent failureEvent = UploadEvent.builder()
                .eventId("failure-test-event")
                .eventType("PROCESSING_FAILED")
                .uploadId(upload.getId())
                .uploaderId(1L)
                .currentStatus(ProcessingStatus.FAILED)
                .errorMessage("Simulated processing failure")
                .errorCode("TEST_FAILURE")
                .eventTime(LocalDateTime.now())
                .build();

        // ì‹¤íŒ¨ ì´ë²¤íŠ¸ë¥¼ ì²˜ë¦¬ ê²°ê³¼ í† í”½ì— ë°œì†¡
        String eventJson = objectMapper.writeValueAsString(failureEvent);
        try (KafkaProducer<String, String> producer = createTestKafkaProducer()) {
            ProducerRecord<String, String> record = new ProducerRecord<>(
                    "test-processing-results",
                    "failure-key",
                    eventJson
            );
            producer.send(record).get();
        }

        // ì‹¤íŒ¨ ì²˜ë¦¬ ê²°ê³¼ í™•ì¸
        await().atMost(10, TimeUnit.SECONDS).untilAsserted(() -> {
            Upload failedUpload = uploadRepository.findById(upload.getId()).orElse(null);
            assertThat(failedUpload).isNotNull();
            assertThat(failedUpload.getProcessingStatus()).isEqualTo(ProcessingStatus.FAILED);
            assertThat(failedUpload.getErrorMessage()).contains("Simulated processing failure");
        });

        System.out.println("âœ… ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤ í™•ì¸ ì™„ë£Œ");

        Upload failedUpload = uploadRepository.findById(upload.getId()).orElse(null);
        System.out.println("ğŸ“Š ì‹¤íŒ¨ ì •ë³´:");
        System.out.println("  - ìƒíƒœ: " + failedUpload.getProcessingStatus());
        System.out.println("  - ì—ëŸ¬ ë©”ì‹œì§€: " + failedUpload.getErrorMessage());
    }

    private KafkaProducer<String, String> createTestKafkaProducer() {
        java.util.Properties props = new java.util.Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("acks", "all");
        return new KafkaProducer<>(props);
    }
}